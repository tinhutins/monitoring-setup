# my global config
global:
  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).
# Alertmanager configuration
alerting:
  alertmanagers:
    - file_sd_configs:
      - files:
        - './targets/alertmanager-target.yml'
        #alertmanager-k8s-master-targets
# BEGIN ANSIBLE MANAGED BLOCK alertmanager cratis-prod-cdc.int.cratis.cloud targets
# Ansible
#       - './targets/alertmanager-cratis-prod-cdc.int.cratis.cloud-targets.yml'
# END ANSIBLE MANAGED BLOCK alertmanager cratis-prod-cdc.int.cratis.cloud targets

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
  - "/etc/prometheus/rules/*"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: "swarm-prometheus"
    scrape_interval: 15s
    # metrics_path defaults to '/metrics'
    # metrics_path: '/prometheus/metrics'
    # scheme defaults to 'http'.
    honor_labels: true
    static_configs:
      - targets: ["prometheus:9090"]
  
  # Create a job for Docker daemons.
  - job_name: 'docker'
    dockerswarm_sd_configs:
      - host: unix:///var/run/docker.sock
        role: nodes
    relabel_configs:
      # Fetch metrics on port 9323.
      - source_labels: [__meta_dockerswarm_node_address]
        target_label: __address__
        replacement: $1:9323
      # Set hostname as instance label
      - source_labels: [__meta_dockerswarm_node_hostname]
        target_label: instance 

          #  - job_name: 'cadvisor'
          #    dns_sd_configs:
          #    - names:
          #      - 'tasks.cadvisor'
          #      type: 'A'
          #      port: 8080
          #
          #  - job_name: 'node-exporter'
          #    dns_sd_configs:
          #    - names:
          #      - 'tasks.node-exporter'
          #      type: 'A'
          #      port: 9100

# Create a job for Docker Swarm containers.
  - job_name: 'dockerswarm_tasks'
    dockerswarm_sd_configs:
      - host: unix:///var/run/docker.sock
        role: tasks
    relabel_configs:
    - source_labels: [__meta_dockerswarm_task_desired_state]
      regex: running
      action: keep
    - source_labels: [__meta_dockerswarm_service_label_prometheus_job]
      regex: .+
      action: keep
    - source_labels: [__address__]
      regex: '^10\.100.*'
      action: keep
    - source_labels: [__meta_dockerswarm_service_label_prometheus_job]
      target_label: job
    # Include labels that start with "__meta_".
    - source_labels: [__meta_dockerswarm_node_id]
      target_label: node_id
    - source_labels: [__meta_dockerswarm_service_name]
      target_label: service_name
    - source_labels: [__meta_dockerswarm_service_label_com_docker_stack_namespace]
      target_label: namespace

remote_write:
  - url: http://victoriametrics:8428/api/v1/write
    remote_timeout: 60s
    follow_redirects: true
    queue_config:
      capacity: 2500
      max_shards: 200
      min_shards: 8
      max_samples_per_send: 500
      batch_send_deadline: 10s
      min_backoff: 60ms
      max_backoff: 10s
